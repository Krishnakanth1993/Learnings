2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO - Logger initialized. Log file: C:\Users\krish\Documents\Krishnakanth\Learnings\Learnings\MNIST_Model\Reference\ERAS7\Model_Evolution\FineTune\logs\20251002_151404_cifar10_training.log
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO - Updated Configuration (from main()):
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Epochs: 200
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Learning Rate: 0.075
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Optimizer: SGD
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Weight Decay: 0.0
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Momentum: 0.9
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Scheduler: ReduceLROnPlateau
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Mode: min
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Factor: 0.5
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Patience: 5
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Threshold: 0.0001
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Batch Size: 128
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Num Workers: 4
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Pin Memory: True
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Shuffle: True
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Dropout Rate: 0.05
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Device: CUDA
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Log Directory: C:\Users\krish\Documents\Krishnakanth\Learnings\Learnings\MNIST_Model\Reference\ERAS7\Model_Evolution\FineTune\logs
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Model Save Directory: C:\Users\krish\Documents\Krishnakanth\Learnings\Learnings\MNIST_Model\Reference\ERAS7\Model_Evolution\FineTune\models
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Save Model: True
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO -   - Log Level: DEBUG
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO - ==================================================
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO - ==================================================
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO - CIFAR-10 TRAINING EXPERIMENT STARTED
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO - ==================================================
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO - Data Config: DataConfig(data_dir='./data', batch_size=128, num_workers=4, pin_memory=True, shuffle=True, mean=(0.1307,), std=(0.3081,), cifar10_mean=(0.4914, 0.4822, 0.4465), cifar10_std=(0.247, 0.2435, 0.2616), rotation_range=(-7.0, 7.0), fill_value=1)
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO - Model Config: ModelConfig(input_channels=1, input_size=(28, 28), num_classes=10, dropout_rate=0.05)
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO - Training Config: TrainingConfig(epochs=200, learning_rate=0.075, momentum=0.9, weight_decay=0.0, scheduler_step_size=30, scheduler_gamma=0.1, seed=1, optimizer_type='SGD', adam_betas=(0.9, 0.999), adam_eps=1e-08, rmsprop_alpha=0.99, scheduler_type='ReduceLROnPlateau', cosine_t_max=20, exponential_gamma=0.95, plateau_mode='min', plateau_factor=0.5, plateau_patience=5, plateau_threshold=0.0001)
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO - ==================================================
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO - Setting up data...
2025-10-02 15:14:04,734 - CIFAR-10_Training - INFO - Using Albumentations for data augmentation
2025-10-02 15:14:04,750 - CIFAR-10_Training - INFO - Loading CIFAR-10 dataset...
2025-10-02 15:14:06,100 - CIFAR-10_Training - INFO - CIFAR-10 dataset loaded successfully!
2025-10-02 15:14:06,100 - CIFAR-10_Training - INFO - Train samples: 50000
2025-10-02 15:14:06,100 - CIFAR-10_Training - INFO - Test samples: 10000
2025-10-02 15:14:06,100 - CIFAR-10_Training - INFO - Augmentation library: Albumentations
2025-10-02 15:14:06,100 - CIFAR-10_Training - INFO - Computing CIFAR-10 data statistics...
2025-10-02 15:14:07,825 - CIFAR-10_Training - INFO - CIFAR-10 Data Statistics:
2025-10-02 15:14:07,825 - CIFAR-10_Training - INFO -   - Shape: (50000, 32, 32, 3)
2025-10-02 15:14:07,825 - CIFAR-10_Training - INFO -   - Size: 153,600,000
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -   - Min: 0.0000
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -   - Max: 1.0000
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -   - Mean: 0.4734
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -   - Std: 0.2516
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -   - Variance: 0.0633
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO - Channel-wise Statistics:
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -   Red Channel:
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -     - Mean: 0.4914
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -     - Std: 0.2470
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -     - Min: 0.0000
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -     - Max: 1.0000
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -   Green Channel:
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -     - Mean: 0.4822
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -     - Std: 0.2435
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -     - Min: 0.0000
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -     - Max: 1.0000
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -   Blue Channel:
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -     - Mean: 0.4465
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -     - Std: 0.2616
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -     - Min: 0.0000
2025-10-02 15:14:07,833 - CIFAR-10_Training - INFO -     - Max: 1.0000
2025-10-02 15:14:22,080 - CIFAR-10_Training - INFO - CIFAR-10 Batch Information:
2025-10-02 15:14:22,080 - CIFAR-10_Training - INFO -   - Batch size: 128
2025-10-02 15:14:22,080 - CIFAR-10_Training - INFO -   - Image shape: torch.Size([3, 32, 32])
2025-10-02 15:14:22,080 - CIFAR-10_Training - INFO -   - Label shape: torch.Size([128])
2025-10-02 15:14:22,080 - CIFAR-10_Training - INFO -   - Data type: torch.float32
2025-10-02 15:14:22,080 - CIFAR-10_Training - INFO -   - Number of classes: 10
2025-10-02 15:14:23,209 - CIFAR-10_Training - INFO - Getting input size from CIFAR-10 data loader...
2025-10-02 15:14:39,309 - CIFAR-10_Training - INFO - CIFAR-10 input size from data loader: (3, 32, 32)
2025-10-02 15:14:40,603 - CIFAR-10_Training - INFO - Setting up model...
2025-10-02 15:14:40,648 - CIFAR-10_Training - INFO - Generating model summary...
2025-10-02 15:14:41,660 - CIFAR-10_Training - INFO - Model Architecture Summary:
2025-10-02 15:14:41,660 - CIFAR-10_Training - INFO -   - Total Parameters: 144,426
2025-10-02 15:14:41,660 - CIFAR-10_Training - INFO -   - Batch Normalization: Yes
2025-10-02 15:14:41,660 - CIFAR-10_Training - INFO -   - Dropout: No
2025-10-02 15:14:41,660 - CIFAR-10_Training - INFO -   - GAP Layers: Yes
2025-10-02 15:14:41,660 - CIFAR-10_Training - INFO -   - FC Layers: Yes
2025-10-02 15:14:41,660 - CIFAR-10_Training - INFO - ==================================================
2025-10-02 15:14:41,660 - CIFAR-10_Training - INFO - DETAILED MODEL ARCHITECTURE SUMMARY
2025-10-02 15:14:41,660 - CIFAR-10_Training - INFO - ==================================================
2025-10-02 15:14:41,660 - CIFAR-10_Training - INFO - ----------------------------------------------------------------
2025-10-02 15:14:41,660 - CIFAR-10_Training - INFO -         Layer (type)               Output Shape         Param #
2025-10-02 15:14:41,660 - CIFAR-10_Training - INFO - ================================================================
2025-10-02 15:14:41,660 - CIFAR-10_Training - INFO -             Conv2d-1           [-1, 32, 32, 32]             864
2025-10-02 15:14:41,660 - CIFAR-10_Training - INFO -        BatchNorm2d-2           [-1, 32, 32, 32]              64
2025-10-02 15:14:41,661 - CIFAR-10_Training - INFO -               ReLU-3           [-1, 32, 32, 32]               0
2025-10-02 15:14:41,661 - CIFAR-10_Training - INFO -             Conv2d-4           [-1, 32, 32, 32]           9,216
2025-10-02 15:14:41,661 - CIFAR-10_Training - INFO -        BatchNorm2d-5           [-1, 32, 32, 32]              64
2025-10-02 15:14:41,661 - CIFAR-10_Training - INFO -               ReLU-6           [-1, 32, 32, 32]               0
2025-10-02 15:14:41,661 - CIFAR-10_Training - INFO -             Conv2d-7           [-1, 32, 32, 32]           9,216
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -        BatchNorm2d-8           [-1, 32, 32, 32]              64
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -               ReLU-9           [-1, 32, 32, 32]               0
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -         Dropout2d-10           [-1, 32, 32, 32]               0
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -            Conv2d-11           [-1, 32, 16, 16]           9,248
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -       BatchNorm2d-12           [-1, 32, 16, 16]              64
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -            Conv2d-13           [-1, 32, 16, 16]           9,216
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -       BatchNorm2d-14           [-1, 32, 16, 16]              64
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -              ReLU-15           [-1, 32, 16, 16]               0
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -            Conv2d-16           [-1, 32, 16, 16]           9,216
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -       BatchNorm2d-17           [-1, 32, 16, 16]              64
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -              ReLU-18           [-1, 32, 16, 16]               0
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -            Conv2d-19           [-1, 32, 16, 16]           9,216
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -       BatchNorm2d-20           [-1, 32, 16, 16]              64
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -              ReLU-21           [-1, 32, 16, 16]               0
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -         Dropout2d-22           [-1, 32, 16, 16]               0
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -            Conv2d-23             [-1, 32, 8, 8]           9,248
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -       BatchNorm2d-24             [-1, 32, 8, 8]              64
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -            Conv2d-25             [-1, 32, 8, 8]             320
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -            Conv2d-26             [-1, 32, 8, 8]           1,056
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO - depthwise_separable_conv-27             [-1, 32, 8, 8]               0
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -       BatchNorm2d-28             [-1, 32, 8, 8]              64
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -              ReLU-29             [-1, 32, 8, 8]               0
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -            Conv2d-30             [-1, 32, 8, 8]           9,216
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -       BatchNorm2d-31             [-1, 32, 8, 8]              64
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -              ReLU-32             [-1, 32, 8, 8]               0
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -            Conv2d-33             [-1, 32, 8, 8]           9,216
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -       BatchNorm2d-34             [-1, 32, 8, 8]              64
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -              ReLU-35             [-1, 32, 8, 8]               0
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -         Dropout2d-36             [-1, 32, 8, 8]               0
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -            Conv2d-37             [-1, 32, 4, 4]           9,248
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -       BatchNorm2d-38             [-1, 32, 4, 4]              64
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -            Conv2d-39             [-1, 32, 4, 4]             320
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -            Conv2d-40             [-1, 64, 4, 4]           2,112
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO - depthwise_separable_conv-41             [-1, 64, 4, 4]               0
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -       BatchNorm2d-42             [-1, 64, 4, 4]             128
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -              ReLU-43             [-1, 64, 4, 4]               0
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -            Conv2d-44             [-1, 64, 4, 4]          36,864
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -       BatchNorm2d-45             [-1, 64, 4, 4]             128
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -              ReLU-46             [-1, 64, 4, 4]               0
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -         Dropout2d-47             [-1, 64, 4, 4]               0
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -         AvgPool2d-48             [-1, 64, 1, 1]               0
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -            Linear-49                  [-1, 128]           8,320
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO -            Linear-50                   [-1, 10]           1,290
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO - ================================================================
2025-10-02 15:14:41,662 - CIFAR-10_Training - INFO - Total params: 144,426
2025-10-02 15:14:41,666 - CIFAR-10_Training - INFO - Trainable params: 144,426
2025-10-02 15:14:41,666 - CIFAR-10_Training - INFO - Non-trainable params: 0
2025-10-02 15:14:41,666 - CIFAR-10_Training - INFO - ----------------------------------------------------------------
2025-10-02 15:14:41,666 - CIFAR-10_Training - INFO - Input size (MB): 0.01
2025-10-02 15:14:41,666 - CIFAR-10_Training - INFO - Forward/backward pass size (MB): 3.54
2025-10-02 15:14:41,666 - CIFAR-10_Training - INFO - Params size (MB): 0.55
2025-10-02 15:14:41,666 - CIFAR-10_Training - INFO - Estimated Total Size (MB): 4.11
2025-10-02 15:14:41,666 - CIFAR-10_Training - INFO - ----------------------------------------------------------------
2025-10-02 15:14:41,666 - CIFAR-10_Training - INFO - ==================================================
2025-10-02 15:14:41,666 - CIFAR-10_Training - INFO - Setting up trainer...
2025-10-02 15:14:41,668 - CIFAR-10_Training - INFO - Using device: cuda
2025-10-02 15:14:41,668 - CIFAR-10_Training - INFO - Early stopping: Stop if train_acc - test_acc > 15.0% for 10 epochs
2025-10-02 15:14:41,668 - CIFAR-10_Training - INFO - Starting training process...
2025-10-02 15:14:41,668 - CIFAR-10_Training - INFO - Starting training process...
2025-10-02 15:14:41,668 - CIFAR-10_Training - INFO - Using optimizer: SGD
2025-10-02 15:14:41,669 - CIFAR-10_Training - INFO - Using scheduler: ReduceLROnPlateau
2025-10-02 15:14:41,669 - CIFAR-10_Training - INFO - Optimizer Configuration:
2025-10-02 15:14:41,669 - CIFAR-10_Training - INFO -   - Learning Rate: 0.075
2025-10-02 15:14:41,669 - CIFAR-10_Training - INFO -   - Momentum: 0.9
2025-10-02 15:14:41,669 - CIFAR-10_Training - INFO -   - Weight Decay: 0.0
2025-10-02 15:14:41,669 - CIFAR-10_Training - INFO - Scheduler Configuration:
2025-10-02 15:14:41,669 - CIFAR-10_Training - INFO -   - Mode: min
2025-10-02 15:14:41,669 - CIFAR-10_Training - INFO -   - Factor: 0.5
2025-10-02 15:14:41,669 - CIFAR-10_Training - INFO -   - Patience: 5
2025-10-02 15:14:41,669 - CIFAR-10_Training - INFO -   - Threshold: 0.0001
2025-10-02 15:14:41,669 - CIFAR-10_Training - INFO - Starting Epoch 1/200
2025-10-02 15:15:34,740 - CIFAR-10_Training - INFO - Epoch  1: Train Loss: 1.8159, Train Acc: 30.15%, Test Loss: 1.5332, Test Acc: 40.02%, Acc Diff: -9.87%, LR: 0.075000
2025-10-02 15:15:34,741 - CIFAR-10_Training - INFO - Starting Epoch 2/200
2025-10-02 15:16:17,330 - CIFAR-10_Training - INFO - Epoch  2: Train Loss: 1.5428, Train Acc: 42.03%, Test Loss: 1.3859, Test Acc: 48.92%, Acc Diff: -6.89%, LR: 0.075000
2025-10-02 15:16:17,330 - CIFAR-10_Training - INFO - Starting Epoch 3/200
2025-10-02 15:17:01,237 - CIFAR-10_Training - INFO - Epoch  3: Train Loss: 1.3741, Train Acc: 49.89%, Test Loss: 1.2098, Test Acc: 56.10%, Acc Diff: -6.21%, LR: 0.075000
2025-10-02 15:17:01,239 - CIFAR-10_Training - INFO - Starting Epoch 4/200
2025-10-02 15:17:42,771 - CIFAR-10_Training - INFO - Epoch  4: Train Loss: 1.2602, Train Acc: 54.95%, Test Loss: 1.1267, Test Acc: 59.95%, Acc Diff: -5.00%, LR: 0.075000
2025-10-02 15:17:42,771 - CIFAR-10_Training - INFO - Starting Epoch 5/200
2025-10-02 15:18:21,598 - CIFAR-10_Training - INFO - Epoch  5: Train Loss: 1.1799, Train Acc: 58.36%, Test Loss: 1.0956, Test Acc: 61.90%, Acc Diff: -3.54%, LR: 0.075000
2025-10-02 15:18:21,598 - CIFAR-10_Training - INFO - Starting Epoch 6/200
2025-10-02 15:19:11,528 - CIFAR-10_Training - INFO - Epoch  6: Train Loss: 1.1196, Train Acc: 60.59%, Test Loss: 0.9643, Test Acc: 66.03%, Acc Diff: -5.44%, LR: 0.075000
2025-10-02 15:19:11,528 - CIFAR-10_Training - INFO - Starting Epoch 7/200
2025-10-02 15:19:49,907 - CIFAR-10_Training - INFO - Epoch  7: Train Loss: 1.0683, Train Acc: 62.59%, Test Loss: 0.9264, Test Acc: 67.76%, Acc Diff: -5.17%, LR: 0.075000
2025-10-02 15:19:49,907 - CIFAR-10_Training - INFO - Starting Epoch 8/200
